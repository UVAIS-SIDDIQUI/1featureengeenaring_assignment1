{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79fb621e-5218-434c-8bd0-5a83a2ad1faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is the Filter method in feature selection, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dd52c7-c77a-4ad2-b185-4957e0b5bb31",
   "metadata": {},
   "source": [
    "ANS = The filter method is one of the techniques used for feature selection in machine learning. It is a preprocessing step where features are selected based on their statistical properties, independently of any machine learning model. This method involves evaluating the relevance of each feature by examining the intrinsic properties of the data, such as correlation with the output variable, and selecting or excluding them accordingly.\n",
    "\n",
    "How the Filter Method Works:-\n",
    "\n",
    "Statistical Metrics: The filter method uses statistical metrics to evaluate the importance of each feature. \n",
    "Common metrics include:\n",
    "\n",
    "Correlation Coefficient: Measures the linear relationship between a feature and the target variable.\n",
    "\n",
    "Mutual Information: Measures the dependency between variables.\n",
    "\n",
    "Chi-Squared Test: Measures the independence between categorical features and the target variable.\n",
    "\n",
    "ANOVA F-Value: Measures the variance between different groups for continuous features.\n",
    "\n",
    "Rank Features: Each feature is scored based on the chosen statistical metric. The features are then ranked \n",
    "according to their scores.\n",
    "\n",
    "Select Features: A subset of features is selected based on the ranking. This can be done by:\n",
    "\n",
    "Selecting the top k features with the highest scores.\n",
    "\n",
    "Selecting features that surpass a certain threshold score.\n",
    "\n",
    "Advantages of the Filter Method\n",
    "\n",
    "Simplicity: It is straightforward and computationally efficient, as it doesn't involve training a model.\n",
    "\n",
    "Speed: Since it evaluates features independently of any model, it is faster compared to wrapper and embedded methods.\n",
    "Independence from Models: The selection process does not depend on any specific learning algorithm, making it more generalizable.\n",
    "\n",
    "\n",
    "Disadvantages of the Filter Method\n",
    "\n",
    "\n",
    "Independence Assumption: It considers each feature independently and does not take into account interactions between features.\n",
    "\n",
    "Potential Overlook of Important Features: Important features that do not show strong individual statistical correlation with the target may be overlooked.\n",
    "\n",
    "\n",
    "\n",
    "EXAMPLE:-\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "\n",
    "data = load_iris()\n",
    "\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "selector = SelectKBest(score_func=f_classif, k=2)  # Select top 2 features\n",
    "\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "\n",
    "print(\"Selected features:\", selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6a6c30b-e6e3-4e25-849b-a839dc9b5339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. How does the Wrapper method differ from the Filter method in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8a36df-b5c4-4309-b891-be2b159c3a60",
   "metadata": {},
   "source": [
    "ANS =The Wrapper method and the Filter method are two different approaches to feature selection in machine learning. Here are the key differences between them:\n",
    "\n",
    "Wrapper Method\n",
    "\n",
    "Model-Based: The Wrapper method uses a predictive model to evaluate the importance of subsets of features.\n",
    "\n",
    "It involves training the model multiple times on different subsets of features and selecting the subset that performs the best according to some evaluation metric (e.g., accuracy, F1 score).\n",
    "\n",
    "Evaluation Process:\n",
    "\n",
    "Forward Selection: Starts with no features and adds features one by one, evaluating the model's performance at each step.\n",
    "\n",
    "Backward Elimination: Starts with all features and removes them one by one, evaluating the model's performance at each step.\n",
    "\n",
    "Recursive Feature Elimination (RFE): Repeatedly constructs the model and removes the least important feature(s).\n",
    "Advantages:\n",
    "\n",
    "Captures Feature Interactions: Since it evaluates subsets of features together, it can capture interactions between features.\n",
    "\n",
    "Model-Specific: It tailors the feature selection to the specific machine learning algorithm being used, potentially improving performance.\n",
    "Disadvantages:\n",
    "\n",
    "Computationally Intensive: Training the model multiple times on different subsets of features can be very time-consuming, especially with large datasets and complex models.\n",
    "Overfitting Risk: There is a higher risk of overfitting since the selection process is based on the performance of the model on the training data.\n",
    "Filter Method\n",
    "\n",
    "Statistical-Based: The Filter method uses statistical techniques to evaluate the relevance of each feature independently of any predictive model. It relies on metrics like correlation coefficients, mutual information, chi-squared tests, and ANOVA F-values.\n",
    "\n",
    "Evaluation Process:\n",
    "\n",
    "Each feature is scored based on its statistical relationship with the target variable.\n",
    "Features are ranked according to their scores.\n",
    "A subset of features is selected based on the rankings, either by choosing the top k features or those exceeding a certain threshold.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Computationally Efficient: Since it evaluates features independently of the model, it is much faster and less resource-intensive than the Wrapper method.\n",
    "\n",
    "Less Overfitting: There is a lower risk of overfitting because the selection is based on general statistical properties, not specific model performance.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Ignores Feature Interactions: It considers each feature independently, potentially missing interactions between features that could be important for model performance.\n",
    "\n",
    "Model-Agnostic: It does not tailor the feature selection to a specific machine learning algorithm, which might result in suboptimal performance for certain models.\n",
    "\n",
    "\n",
    "EXAMPLE:- Wrapper Method Example (using Recursive Feature Elimination):\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "data = load_iris()\n",
    "\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "model = LogisticRegression(max_iter=200)\n",
    "\n",
    "rfe = RFE(estimator=model, n_features_to_select=2)\n",
    "\n",
    "X_rfe = rfe.fit_transform(X, y)\n",
    "\n",
    "selected_features_rfe = X.columns[rfe.support_]\n",
    "\n",
    "print(\"Selected features (Wrapper Method):\", selected_features_rfe)\n",
    "\n",
    "\n",
    "\n",
    "Filter Method Example (using SelectKBest):\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "data = load_iris()\n",
    "\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "selector = SelectKBest(score_func=f_classif, k=2)\n",
    "\n",
    "X_kbest = selector.fit_transform(X, y)\n",
    "\n",
    "selected_features_kbest = X.columns[selector.get_support()]\n",
    "\n",
    "print(\"Selected features (Filter Method):\", selected_features_kbest)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1797411a-20f1-4e62-96bc-901fdd8e5d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. What are some common techniques used in Embedded feature selection methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eef90a-96e4-42b6-b165-b78614384a0d",
   "metadata": {},
   "source": [
    "ANS = Embedded feature selection methods incorporate the feature selection process within the model training process. These methods perform feature selection during the training phase and often leverage the learning algorithm itself to determine which features contribute most to the model's predictive power. Here are some common techniques used in embedded feature selection methods:\n",
    "\n",
    "1. Regularization Methods\n",
    "\n",
    "Regularization techniques add a penalty term to the loss function to shrink the coefficients of less important features, effectively performing feature selection. Common regularization methods include:\n",
    "\n",
    "Lasso Regression (L1 Regularization): Adds an L1 penalty term to the loss function, which can shrink some coefficients to zero. Features with zero coefficients are excluded from the model.\n",
    "\n",
    "-->\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=0.01)  # alpha is the regularization strength\n",
    "\n",
    "lasso.fit(X, y)\n",
    "\n",
    "selected_features = X.columns[lasso.coef_ != 0]\n",
    "\n",
    "\n",
    "\n",
    "Ridge Regression (L2 Regularization): Adds an L2 penalty term, which shrinks coefficients but does not set them to zero. While it doesn't perform feature selection directly, it can be combined with other methods to improve stability\n",
    "\n",
    "\n",
    "-->\n",
    "\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge(alpha=0.01)\n",
    "\n",
    "ridge.fit(X, y)\n",
    "\n",
    "\n",
    "Elastic Net (Combination of L1 and L2 Regularization): Combines both L1 and L2 penalties. It can select features (like Lasso) and maintain stability (like Ridge).\n",
    "\n",
    "\n",
    "-->\n",
    "\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "elastic_net = ElasticNet(alpha=0.01, l1_ratio=0.5)\n",
    "\n",
    "elastic_net.fit(X, y)\n",
    "\n",
    "selected_features = X.columns[elastic_net.coef_ != 0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. Tree-Based Methods\n",
    "\n",
    "Tree-based models can inherently perform feature selection by evaluating feature importance during the training process. These methods include:\n",
    "\n",
    "Decision Trees: Feature importance can be derived from the Gini impurity or information gain used to split the nodes.\n",
    "\n",
    "-->\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "tree.fit(X, y)\n",
    "\n",
    "importances = tree.feature_importances_\n",
    "\n",
    "selected_features = X.columns[importances > threshold]  # Define a threshold for importance\n",
    "\n",
    "Random Forests: Aggregates the feature importances from multiple decision trees to determine the overall importance of each feature.\n",
    "\n",
    "\n",
    "-->\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier()\n",
    "\n",
    "forest.fit(X, y)\n",
    "\n",
    "importances = forest.feature_importances_\n",
    "\n",
    "selected_features = X.columns[importances > threshold]\n",
    "\n",
    "\n",
    "Gradient Boosting Machines (GBM): Like random forests, GBMs aggregate feature importances over multiple boosting iterations.\n",
    "\n",
    "-->\n",
    "\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbm = GradientBoostingClassifier()\n",
    "\n",
    "gbm.fit(X, y)\n",
    "\n",
    "importances = gbm.feature_importances_\n",
    "\n",
    "selected_features = X.columns[importances > threshold]\n",
    "\n",
    "\n",
    "3. Embedded Methods in Linear Models\n",
    "\n",
    "Some linear models, especially those with sparsity-inducing norms, perform feature selection during training. Examples include:\n",
    "\n",
    "Logistic Regression with L1 Penalty: Similar to Lasso, Logistic Regression with an L1 penalty can shrink some feature coefficients to zero.\n",
    "\n",
    "-->\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "\n",
    "logistic.fit(X, y)\n",
    "\n",
    "selected_features = X.columns[logistic.coef_[0] != 0]\n",
    "\n",
    "\n",
    "4. Regularized Neural Networks\n",
    "\n",
    "Neural networks can also perform feature selection through regularization techniques such as L1 regularization applied to the weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b278f30-bfec-4df3-a899-2442ba07242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. What are some drawbacks of using the Filter method for feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1de3a9-4793-4363-8b88-7f425bf139c2",
   "metadata": {},
   "source": [
    "ANS = The Filter method for feature selection has several drawbacks despite its simplicity and computational efficiency. Here are some of the key drawbacks:\n",
    "\n",
    "Independence Assumption:\n",
    "\n",
    "Filter methods assess the relevance of each feature independently of the others. This means they do not consider interactions between features. Consequently, a set of individually relevant features may not perform well together in a model.\n",
    "\n",
    "Ignoring Model-Specific Performance:\n",
    "\n",
    "Filter methods do not consider the performance of the features within the context of a specific machine learning model. They rely solely on statistical properties like correlation with the target variable, which might not translate into better model performance.\n",
    "\n",
    "Overlooking Redundancy:\n",
    "\n",
    "These methods can select redundant features that provide overlapping information. For example, two features might both be highly correlated with the target variable, but they might also be highly correlated with each other, offering little additional benefit when included together.\n",
    "\n",
    "Simple Metrics:\n",
    "\n",
    "Filter methods often use simple metrics such as correlation coefficient, chi-square test, or mutual information, which may not capture complex relationships between features and the target variable.\n",
    "\n",
    "Risk of Missing Useful Features:\n",
    "\n",
    "Features that are weakly correlated with the target variable on their own but are useful when combined with other features may be discarded by filter methods. This could lead to the exclusion of features that are important in a multivariate context.\n",
    "\n",
    "No Feedback from Model:\n",
    "\n",
    "Since filter methods operate before model training, they do not benefit from feedback on feature importance derived from the actual model. This means they cannot adjust based on how well features actually help the model perform.\n",
    "\n",
    "Static Nature:\n",
    "\n",
    "Filter methods do not adapt based on model performance during training. Once a subset of features is selected, it remains static, potentially ignoring useful dynamic adjustments.\n",
    "\n",
    "Limited to Certain Types of Data:\n",
    "\n",
    "Some filter methods are specific to certain types of data or distributions (e.g., chi-square tests are applicable to categorical data), limiting their applicability across diverse datasets without modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07b77cda-e51b-4429-997e-14f52e6d891b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. In which situations would you prefer using the Filter method over the Wrapper method for feature \n",
    "# selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1d3732-4a69-44f8-b2d3-a7c9b399e7f7",
   "metadata": {},
   "source": [
    "ANS = The Filter method for feature selection is preferable over the Wrapper method in several situations:\n",
    "\n",
    "High Dimensionality:\n",
    "\n",
    "When dealing with datasets that have a very large number of features (e.g., thousands or more), the Filter method is more computationally efficient. Wrapper methods, which involve training and evaluating models for different subsets of features, can be prohibitively slow and resource-intensive in such scenarios.\n",
    "\n",
    "Preprocessing Step:\n",
    "\n",
    "Filter methods can be used as a preliminary step to reduce the number of features before applying more computationally intensive methods like Wrapper or Embedded methods. This can help to initially trim down the feature set to a more manageable size.\n",
    "\n",
    "Simplicity and Speed:\n",
    "\n",
    "If the primary concern is to quickly reduce the dimensionality of the dataset with a straightforward approach, the Filter method is ideal. It is fast because it doesn't involve training a model for each subset of features.\n",
    "\n",
    "Independence from Algorithm:\n",
    "\n",
    "Filter methods are independent of the machine learning algorithm to be used. If there is a need to perform feature selection without committing to a specific model, Filter methods provide a good option since they rely on general statistical properties of the data.\n",
    "\n",
    "Baseline Feature Selection:\n",
    "\n",
    "In initial stages of data exploration and analysis, the Filter method can serve as a baseline to understand the relevance of features before diving into more complex methods.\n",
    "\n",
    "Avoiding Overfitting:\n",
    "\n",
    "Because Filter methods do not involve iterative model training, they are less prone to overfitting compared to Wrapper methods, which can overfit the training data due to repeated model evaluations on different subsets of features.\n",
    "\n",
    "Interpreting Feature Importance:\n",
    "\n",
    "Filter methods often use clear, interpretable metrics (such as correlation coefficients, chi-square statistics, or mutual information), which can provide intuitive insights into the relevance of individual features.\n",
    "\n",
    "Resource Constraints:\n",
    "\n",
    "When computational resources (time, memory, processing power) are limited, Filter methods provide a practical solution for feature selection without the heavy resource demands of Wrapper methods.\n",
    "\n",
    "Initial Data Cleaning:\n",
    "\n",
    "In scenarios where the dataset is large and noisy, Filter methods can help in quickly eliminating irrelevant or low-variance features, providing a cleaner dataset for further, more refined analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c472f70-fb17-4e6c-a833-1ac3aa550b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. In a telecom company, you are working on a project to develop a predictive model for customer churn. \n",
    "# You are unsure of which features to include in the model because the dataset contains several different \n",
    "# ones. Describe how you would choose the most pertinent attributes for the model using the Filter Method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbb836a-50f1-4cb6-a994-9d87ecc69862",
   "metadata": {},
   "source": [
    "To choose the most pertinent attributes for a customer churn predictive model in a telecom company using the Filter Method,\n",
    "\n",
    "you can follow these steps:\n",
    "\n",
    "Understand the Data:\n",
    "\n",
    "Begin with a thorough understanding of the dataset, including the various features available, their types (numerical, categorical, etc.), and their relevance to customer churn.\n",
    "\n",
    "Data Preprocessing:\n",
    "\n",
    "Clean the data: Handle missing values, outliers, and noise.\n",
    "\n",
    "Encode categorical variables: Use techniques like one-hot encoding or label encoding to convert categorical variables into a numerical format.\n",
    "\n",
    "Select Statistical Metrics:\n",
    "\n",
    "Choose appropriate statistical metrics to evaluate the relevance of each feature with respect to the target \n",
    "\n",
    "variable (customer churn). Common metrics include:\n",
    "\n",
    "For numerical features: Pearson correlation coefficient.\n",
    "\n",
    "For categorical features: Chi-square test or mutual information.\n",
    "\n",
    "For mixed types: ANOVA F-value or mutual information.\n",
    "\n",
    "Compute Feature Relevance:\n",
    "\n",
    "Calculate the chosen statistical metric for each feature with respect to the target variable. For example:\n",
    "\n",
    "Pearson correlation coefficient: Measures linear correlation between numerical features and the churn variable.\n",
    "\n",
    "Chi-square test: Assesses the independence between categorical features and the churn variable.\n",
    "\n",
    "Mutual information: Measures the amount of information shared between each feature and the churn variable.\n",
    "\n",
    "Rank Features:\n",
    "\n",
    "Rank the features based on their statistical scores. Higher scores indicate a stronger relationship with the target variable (churn).\n",
    "\n",
    "Select Top Features:\n",
    "\n",
    "Select the top N features based on their ranks. The value of N can be determined based on cross-validation performance or domain knowledge. For example, you might start with the top 10-20 features and then fine-tune further.\n",
    "\n",
    "Validate Feature Selection:\n",
    "\n",
    "Validate the chosen features by:\n",
    "\n",
    "Cross-validation: Perform cross-validation using the selected features to ensure they improve the predictive performance of the model.\n",
    "\n",
    "Domain expertise: Consult with domain experts to verify that the selected features make sense from a business perspective.\n",
    "\n",
    "Example of Applying the Filter Method\n",
    "\n",
    "Data Understanding:\n",
    "\n",
    "Features: customer_age, tenure, monthly_charge, contract_type, payment_method, service_usage, complaints, etc.\n",
    "\n",
    "Target variable: churn (binary: 0 for no churn, 1 for churn).\n",
    "\n",
    "Data Preprocessing:\n",
    "\n",
    "Handle missing values and outliers.\n",
    "\n",
    "Encode categorical variables like contract_type, payment_method.\n",
    "\n",
    "Select Statistical Metrics:\n",
    "\n",
    "For numerical features: Pearson correlation coefficient.\n",
    "\n",
    "For categorical features: Chi-square test.\n",
    "\n",
    "For mixed types: Mutual information.\n",
    "\n",
    "Compute Feature Relevance:\n",
    "\n",
    "Calculate Pearson correlation for numerical features:\n",
    "\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "corr_scores = {feature: pearsonr(data[feature], data['churn'])[0] for feature in numerical_features}\n",
    "\n",
    "\n",
    "Calculate Chi-square test for categorical features:\n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "chi2_scores, _ = chi2(data[categorical_features], data['churn'])\n",
    "\n",
    "chi2_scores = dict(zip(categorical_features, chi2_scores))\n",
    "\n",
    "Calculate mutual information for mixed features:\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "mi_scores = mutual_info_classif(data[features], data['churn'])\n",
    "\n",
    "mi_scores = dict(zip(features, mi_scores))\n",
    "\n",
    "Rank Features:\n",
    "\n",
    "Rank the features based on their scores from the selected metrics.\n",
    "Select Top Features:\n",
    "\n",
    "Choose the top N features with the highest scores.\n",
    "\n",
    "Validate Feature Selection:\n",
    "\n",
    "Perform cross-validation and consult with domain experts to validate the relevance of the selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "643ebe85-596d-40d0-a033-35d81546cd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. You are working on a project to predict the outcome of a soccer match. You have a large dataset with \n",
    "# many features, including player statistics and team rankings. Explain how you would use the Embedded \n",
    "# method to select the most relevant features for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24da71ac-c472-4881-b879-3eebf222f5d8",
   "metadata": {},
   "source": [
    "ANS = To use the Embedded method for feature selection in predicting the outcome of a soccer match, you would integrate feature selection as part of the model training process. Embedded methods not only select features but also take into account the interactions between them, making them highly effective for complex datasets. Here's a \n",
    "step-by-step guide:\n",
    "\n",
    "Step-by-Step Guide to Using Embedded Methods\n",
    "\n",
    "Understand the Data:\n",
    "\n",
    "Familiarize yourself with the dataset, which includes features like player statistics (goals, assists, tackles, passes), team rankings, historical match outcomes, etc.\n",
    "\n",
    "Data Preprocessing:\n",
    "\n",
    "Clean the data: Handle missing values, outliers, and anomalies.\n",
    "\n",
    "Encode categorical variables: Convert categorical variables (e.g., player positions, team names) into numerical formats using techniques like one-hot encoding or label encoding.\n",
    "\n",
    "Normalize/scale the data: Ensure features are on a similar scale, which is important for many machine learning algorithms.\n",
    "\n",
    "Select a Suitable Model:\n",
    "\n",
    "Choose a machine learning model that supports feature importance calculation as part of the training process. \n",
    "\n",
    "Common models for embedded feature selection include:\n",
    "\n",
    "Regularization-based models: Lasso (L1 regularization) or Ridge (L2 regularization) regression.\n",
    "\n",
    "Tree-based models: Decision Trees, Random Forests, Gradient Boosting Machines (GBMs), or XGBoost.\n",
    "\n",
    "Train the Model with Embedded Feature Selection:\n",
    "\n",
    "Regularization-based models: Apply Lasso or Ridge regression, which penalizes the absolute size of the \n",
    "\n",
    "coefficients, driving some of them to zero. This process inherently selects the most important features.\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=0.01)  # Adjust alpha for regularization strength\n",
    "\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "Tree-based models: Train models like Random Forest or Gradient Boosting that inherently provide feature importance scores based on how often and how effectively features are used to split the data.\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "Extract Feature Importances:\n",
    "\n",
    "For Lasso Regression\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "feature_importances = np.abs(lasso.coef_)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "feature_importances = np.abs(lasso.coef_)\n",
    "\n",
    "For Tree-based Models:\n",
    "\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "Rank and Select Features:\n",
    "\n",
    "Rank the features based on their importance scores.\n",
    "Select the top N features with the highest importance scores. The exact number of features can be determined based on cross-validation performance or domain expertise.\n",
    "\n",
    "Validate the Selected Features:\n",
    "\n",
    "Cross-validation: Perform cross-validation to ensure that the selected features improve model performance.\n",
    "\n",
    "Model performance evaluation: Compare the performance of models using all features versus the selected features \n",
    "based on metrics like accuracy, precision, recall, F1-score, etc.\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(rf, X_train[:, top_features], y_train, cv=5)\n",
    "\n",
    "print(scores.mean())\n",
    "\n",
    "\n",
    "Iterate and Fine-Tune:\n",
    "\n",
    "Iterate the process, adjusting the model parameters and the number of selected features to fine-tune the model's performance.\n",
    "Example Using Embedded Methods\n",
    "\n",
    "Understand the Data:\n",
    "\n",
    "Features: player_goals, player_assists, team_rank, recent_performance, home_advantage, etc.\n",
    "\n",
    "Target variable: match_outcome (binary or multi-class: win, lose, draw).\n",
    "\n",
    "Data Preprocessing:\n",
    "\n",
    "Clean and encode the data, normalize if necessary.\n",
    "\n",
    "Select a Suitable Model:\n",
    "\n",
    "Use RandomForestClassifier for its inherent feature importance calculation.\n",
    "\n",
    "Train the Model:\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "Extract Feature Importances:\n",
    "\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "\n",
    "Rank and Select Features:\n",
    "\n",
    "importances = pd.Series(feature_importances, index=feature_names)\n",
    "\n",
    "top_features = importances.nlargest(N).index\n",
    "\n",
    "Validate the Selected Features:\n",
    "\n",
    "X_train_selected = X_train[top_features]\n",
    "\n",
    "scores = cross_val_score(rf, X_train_selected, y_train, cv=5)\n",
    "\n",
    "print(\"Cross-validation score with selected features: \", scores.mean())\n",
    "\n",
    "Iterate and Fine-Tune:\n",
    "\n",
    "Adjust the number of features and re-evaluate model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "259cda04-3d0c-4a45-ad8b-3ea09c9cfab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. You are working on a project to predict the price of a house based on its features, such as size, location, \n",
    "# and age. You have a limited number of features, and you want to ensure that you select the most important \n",
    "# ones for the model. Explain how you would use the Wrapper method to select the best set of features for the \n",
    "# predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1335f6d-1265-4f87-9e6e-99dd8895aa6d",
   "metadata": {},
   "source": [
    "ANS = The Wrapper method involves selecting features based on their contribution to the performance of a specific machine learning model. It is an iterative process where different combinations of features are evaluated using a predictive model, and the best-performing subset is chosen. Here's how you would use the Wrapper method to select\n",
    "\n",
    "the best set of features for predicting house prices:\n",
    "\n",
    "Step-by-Step Guide to Using the Wrapper Method\n",
    "\n",
    "Understand the Data:\n",
    "\n",
    "Features include size, location, age, number of rooms, amenities, etc.\n",
    "Target variable is the house price.\n",
    "\n",
    "Data Preprocessing:\n",
    "\n",
    "Clean the data: Handle missing values, outliers, and ensure data consistency.\n",
    "\n",
    "Encode categorical variables: Convert categorical features (e.g., location) into numerical format using techniques like one-hot encoding or label encoding.\n",
    "\n",
    "Normalize/scale the data: Ensure features are on a similar scale, which can be important for certain models.\n",
    "\n",
    "Choose a Wrapper Method Strategy:\n",
    "\n",
    "Common strategies include Forward Selection, Backward Elimination, and Recursive Feature Elimination (RFE).\n",
    "\n",
    "For illustration, we'll use Recursive Feature Elimination with Cross-Validation (RFECV).\n",
    "\n",
    "Select a Suitable Model:\n",
    "\n",
    "Choose a regression model that you plan to use for predicting house prices. Common choices include Linear Regression, Decision Trees, Random Forests, or Gradient Boosting Machines.\n",
    "\n",
    "Perform Recursive Feature Elimination with Cross-Validation (RFECV):\n",
    "\n",
    "Initialize the model: Start with your chosen regression model.\n",
    "\n",
    "Apply RFECV: RFECV combines RFE with cross-validation to find the optimal number of features by recursively\n",
    "\n",
    "removing the least important features and evaluating model performance.\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Initialize the model\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "# Set up RFECV with cross-validation\n",
    "\n",
    "rfecv = RFECV(estimator=model, step=1, cv=KFold(5), scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the model\n",
    "rfecv.fit(X_train, y_train)\n",
    "\n",
    "# Get the optimal number of features\n",
    "\n",
    "optimal_num_features = rfecv.n_features_\n",
    "\n",
    "print(f\"Optimal number of features: {optimal_num_features}\")\n",
    "\n",
    "# Get the selected features\n",
    "\n",
    "selected_features = X_train.columns[rfecv.support_]\n",
    "\n",
    "print(f\"Selected features: {selected_features}\")\n",
    "\n",
    "Evaluate Model Performance:\n",
    "\n",
    "Cross-validation: Assess the performance of the model using the selected features through cross-validation to ensure they improve predictive accuracy.\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X_train_selected = X_train[selected_features]\n",
    "\n",
    "scores = cross_val_score(model, X_train_selected, y_train, cv=KFold(5), scoring='neg_mean_squared_error')\n",
    "\n",
    "print(f\"Cross-validation MSE: {-scores.mean()}\")\n",
    "\n",
    "Validate and Fine-Tune:\n",
    "\n",
    "Model evaluation: Compare the performance of the model with the selected features against the performance using all features to ensure that the selection improves or maintains predictive accuracy.\n",
    "Hyperparameter tuning: If necessary, fine-tune the hyperparameters of the regression model for optimal performance with the selected features.\n",
    "Example Using the Wrapper Method\n",
    "Understand the Data:\n",
    "\n",
    "Features: size, location, age, num_rooms, garage, garden, near_school, etc.\n",
    "\n",
    "Target variable: house_price.\n",
    "\n",
    "Data Preprocessing:\n",
    "\n",
    "Clean, encode, and normalize the data as needed.\n",
    "\n",
    "Choose a Wrapper Method Strategy:\n",
    "\n",
    "Use RFECV for feature selection.\n",
    "\n",
    "Select a Suitable Model:\n",
    "\n",
    "Use Linear Regression for illustration.\n",
    "\n",
    "Perform RFECV:\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "rfecv = RFECV(estimator=model, step=1, cv=KFold(5), scoring='neg_mean_squared_error')\n",
    "\n",
    "rfecv.fit(X_train, y_train)\n",
    "\n",
    "optimal_num_features = rfecv.n_features_\n",
    "\n",
    "selected_features = X_train.columns[rfecv.support_]\n",
    "\n",
    "print(f\"Optimal number of features: {optimal_num_features}\")\n",
    "\n",
    "print(f\"Selected features: {selected_features}\")\n",
    "\n",
    "Evaluate Model Performance:\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X_train_selected = X_train[selected_features]\n",
    "\n",
    "scores = cross_val_score(model, X_train_selected, y_train, cv=KFold(5), scoring='neg_mean_squared_error')\n",
    "\n",
    "print(f\"Cross-validation MSE: {-scores.mean()}\")\n",
    "\n",
    "\n",
    "Validate and Fine-Tune:\n",
    "\n",
    "Ensure that the selected features provide the best performance and consider tuning the model further for improved accuracy.\n",
    "\n",
    "By following these steps, you can effectively use the Wrapper method to select the most important features for predicting house prices, ensuring that your model is both efficient and accurate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
